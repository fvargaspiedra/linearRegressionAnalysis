---
title: "Final Project"
author: "Hari Manan (NetID: nfnh2), Javier Matias (NetID: javierm4), Francisco Vargas (NetID: fav3)"
date: 'August 4th, 2018'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

## Students names

- Hari Manan (NetID: nfnh2)
- Javier Matias (NetID: javierm4)
- Francisco Vargas (NetID: fav3)

## Title

Prediction of House Prices in Ames, Iowa Using Regression Analysis Techniques

## Introduction

The main goal of this project is to develop a model to predict the sale price of a house using a dataset gathered on Ames, Iowa related to houses sold between 2006 and 2010. The model not only should be good at predicting the price, but also avoid using all the features provided in order to reduce the complexity of explaining it. In other words, we are aiming to get a model with a balance between prediction and explanation.

There are also practical questions that will be answered which can help a seller or buyer to understand specific characteristics that are important when determining the price of a house.

The dataset originally contains 2930 observations with 81 features (23 nominal/categorical, 23 ordinal, 14 discrete, and 21 continuous).

The features can be roughly divided into 6 categories. We include some examples of each category:

- **House characteristics**: e.g. number of bedrooms, number of bathrooms, electrical system
- **Space measurements**: e.g. lot area, living area
- **Quality ratings**: e.g. overall condition, kitchen condition
- **Construction characteristics and materials**: e.g. roof material
- **Zone and location classification**: e.g. zoning classification, neighborhood
- **Terms of sale**: e.g. month and year sold, sale price

This dataset was compiled by Dean DeCock in 2011 to be used for Data Science education and research. It was an effort towards expanding a famous dataset about the housing market in Boston by adding variables to those already provided in the Boston Housing dataset.

The link to the original paper can be found here: https://ww2.amstat.org/publications/jse/v19n3/decock.pdf

The dataset is also openly available here: http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls

There are several reasons why the team decided to use this dataset and why this is interesting to us. Most of them are related to research and personal motivations and are enumerated below:

1. **Model simplification**: This dataset offers many variables. Even though accuracy tends to improve as the number of predictors increases, larger models also increase complexity and become more difficult to interpret. Complex models are also highly succeptible to overfitting. The goal of this project is to derive a model using a subset of the predictor variables while achieving a comparable level of performance.
2. **Cleanness**: The dataset is well curated and does not require extensive cleansing. This will allow the team to focus on analyzing the data itself.
3. **Questions**: From a personal interest perspective, this dataset is related to a frequent activity: buying a house. This is a well-known activity that allows for some interesting questions:
    - **How effectively can we predict the price of a house given a set of explanatory variables?** A test/training set evaluation process can help us measure the accuracy under acceptable error ranges. This could give us some insight to answer other questions such as: *"Does the time of year affect the sale price of the house?"*
    - **Can we identify unusual observations that might be affecting the model?** Understanding leverage, outliers, and influential points in the model will allow us to identify unusual observations in our regressions model.
    - **Are there any interactions between the variables affecting the price of the houses?** Experimenting with dummy variables and interaction models might help answer questions such as: *"Do some neighborhoods value having a pool or a garage more than others?"*
    - **What are the most influential predictors?** Hypothesis tests might be the best path in this case.
    - **Can we build a reduced version of the model without sacrificing accuracy?** Use variable selection procedures, collinearity, and ANOVA evaluation to find a good model for the housing dataset from a set of possible models.
    - **Are transformations necessary to improve the accuracy of the model?** Transformation techniques and assumption tests might lead us to the answer.

## Method

This section is devoted to execute all the calculations and operations necessary to achieve the goals exposed in the introductory section.

The first step is to load the original data that is attached to this project in the `data` directory:

```{r message=FALSE}
library(readr)

# Load the data
housing_data = read_csv("data/AmesHousing.csv")
```

We are using a 'csv' file that was derived from the original 'xls' file just to use the well known `read_csv` function.

### Cleaning the data

There are two useless variables. `PID` is a unique identifier of each observation and `Order` is a counter for each observation. Let's get rid of those:

```{r}
housing_data = subset(housing_data, select = -Order)
housing_data = subset(housing_data, select = -PID)
```

Let's start by understanding the structure of the original data and clean as needed.

```{r}
# Display structure of dataset
str(housing_data)
```

From the previous output one can see that this dataset is composed by 2930 observations and 82 variables (80 when removing the two useless variables). There are only `chr` and `int` type variables. A description of each variable can be found here https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt. 

It's important to know whether there are `NA` values in the dataset and decide how to manage them. Let's see if there are variables containing `NA` values:

```{r}
hasNA = colSums(is.na(housing_data))[colSums(is.na(housing_data)) > 0]
hasNA
```

As you can see `r length(hasNA)` variables contain `NA` values. Some of those variables have too many `NA`s, therefore we decided to remove any variable with more than 400 `NA`s:

```{r}
# Keep only those variables that has less than 400 NAs 
#housing_data = housing_data[ , names(hasNA[hasNA < 400])]
housing_data = subset(housing_data, select = colSums(is.na(housing_data)) < 400)
```

After filtering, we now have `r dim(housing_data)[1]` observations and `r dim(housing_data)[2]` variables.

In order to avoid any issue with the remaining observations that have `NA`s we are removing those as well:

```{r}
housing_data = na.omit(housing_data)
```

After filtering, we now have `r dim(housing_data)[1]` observations and `r dim(housing_data)[2]` variables. These are enough observations to work on our model without any problem.

There are 5 unusual observations that were pointed out by the author of the dataset. These are easily observable by plotting `SalePrice` (price of the house) vs `Gr Liv Area` (above ground living area square feet) as follows:

```{r}
plot(housing_data$SalePrice ~ housing_data$`Gr Liv Area`,
     col = ifelse(housing_data$`Gr Liv Area` > 4000, "orange", "dodgerblue"),
     xlab = "Ground Living Area (square feet)",
     ylab = "Sale Price (dollars)",
     main = "Sale Price vs Ground Living Area",
     pch = 20
     )
```

As you can see there are 3 points that have an extremely low price for a huge ground living area, this doesn't make sense. The other two points seem to be priced correctly but those are unusual sales. Let's remove them:

```{r}
housing_data = housing_data[housing_data$`Gr Liv Area` < 4000, ]
```

Finally, let's coerce all character columns as factors to fit the categorical predictors of the model without problem:

```{r}
# Determine variables that are of type character
char_var = lapply(housing_data, class) == "character"
# Coerce those columns to factor
housing_data[, char_var] = lapply(housing_data[, char_var], as.factor)
```

### Baseline analysis

As stated in the `Introduction` the goal of the project is to obtain a model for predicting sales price, but trying to keep it simple. In other words, finding a good balance between prediction and explanation.

Before playing around with model selection, let's divide the data into a training and a test set. That will help us evaluate the final model chosen. We'll use the seed `420` for any random process so we can reproduce the results if needed.

```{r}
set.seed(420)

# Get training indexes randomly (80% of the observations)
train_index <- sample(seq_len(nrow(housing_data)), size = floor(0.8 * nrow(housing_data)))

# Divide the data
train_hd <- housing_data[train_index, ]
test_hd <- housing_data[-train_index, ]
```

After splitting the data we now have `r dim(train_hd)[1]` observations for training and `r dim(test_hd)[1]` observations for test.

In order to assess our models we are going to use the set of auxiliary functions defined on week's 9 assignment. We are also defining a macro function that calls all those auxiliary functions and show the results in a single call. This overview encompasses the p-value of the normality and constant variance assumptions through Shapiro and Breusch test, the Fitted vs Residuals plot, the normal Q-Q plot, the number of parameters used (betas), the LOOCV RMSE, and the adjusted $R^2$.

```{r}
# Library required for Breusch-Pagan test
library(lmtest)

# Plot Fitted vs Residuals
plot_fitted_resid = function(model) {
  plot(fitted(model), resid(model), 
       col = "dodgerblue", pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals",
       main = "Fitted vs residuals plot")
  abline(h = 0, col = "darkorange", lwd = 1)
}

# Plot Normal Q-Q
plot_qq = function(model) {
  qqnorm(resid(model), col = "dodgerblue", pch = 20, cex = 1.5)
  qqline(resid(model), col = "darkorange", lwd = 1)
}

# Breusch-Pagan test (constant variance)
get_bp_decision = function(model) {
  bptest(model)$p.value
}

# Shapiro-Wilk test (normality)
get_sw_decision = function(model) {
  shapiro.test(resid(model))$p.value
}

# Number of parameters
get_num_params = function(model) {
  length(coef(model))
}

# LOOCV RMSE
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# Adjusted R^2
get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

# Function that combines the previously defined functions
get_overview = function(model) {
  plot_fitted_resid(model)
  plot_qq(model)
  loocv_rmse = get_loocv_rmse(model)
  adj_r2 = get_adj_r2(model)
  bp_p_value = get_bp_decision(model)
  shapiro_p_value = get_sw_decision(model)
  betas = get_num_params(model)
  list(loocv_rmse = loocv_rmse, adj_r2 = adj_r2, bp_p_value = bp_p_value, shapiro_p_value = shapiro_p_value, betas = betas)
}
```

We can start the process of finding our model by using an additive model as a baseline:

```{r}
model_add = lm(SalePrice ~ ., data = train_hd)
```

Let's run the overview function to assess this baseline model:

```{r}
get_overview(model_add)
```

From the results above you can note that there are troubles on every resulting parameter, except the adjusted $R^2$. LOOCV RMSE is `inf`, both assumptions are violated (p-values are very low), and the number of parameters used is huge (because of the categorical variables, we end up having more betas than the number of predictors available). The Fitted vs Residuals plot also seems to violate both, the 0 mean at any fitted value and the constant variance at any fitted value (no linearity and no constant variance).

Based on the normal Q-Q plot is evident that there is a huge deviation at the edges of the plot. We can apply a log transformation on the response and check whether this minimizes the effect:

```{r}
model_add_log = lm(log(SalePrice) ~ ., data = train_hd)
get_overview(model_add_log)
```

The Shapiro's p-value is worse, but the normal Q-Q plot looks better. This can be due to some outliers that can be observed on the plot. 

The Fitted vs Residuals plot also looks better, especially the 0 mean at any fitted value. There are still some outliers and the variance seems to change for high fitted values.

We tried to find outliers or influential points, but using the heuristics of the book is not possible to find any:

```{r}
# Number of influential points in the transformed model
sum(cooks.distance(model_add_log) > 4 / length(cooks.distance(model_add_log)))

# Number of outliers in the transformed model
sum(abs(rstandard(model_add_log)) > 2)
```

We can also see the positive effect of the `log` function in the response by looking at its histogram. Without `log`:

```{r}
hist(train_hd$SalePrice, 
     breaks = 30,
     main = "Histogram of Sale Price without transformation",
     xlab = "Sale Price",
     col = "gray")
```

As you can see this plot is right skewed. This can be normalized by using `log`:

```{r}
hist(log(train_hd$SalePrice), 
     breaks = 30,
     main = "Histogram of Sale Price with log transformation",
     xlab = "Sale Price",
     col = "gray")
```

We can notice that the obtained plot is much better and the `log transformation` makes sense for the response.

Let's use this transformed additive model as our baseline. There are still many paramaters that can be improved.

### Predictor selection

There are many predictors to play with. We can try to choose a smaller base of "relevant" predictors and then assess a reduction through backward selection process.

One way of doing this is by selecting those numerical predictors that are highly correlated to the response. 

### Practical questions derived from the dataset



## References

1. De Cock, Dean. (2011). *Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project*. Journal of Statistics Education.
2. *House Prices: Advanced Regression Techniques*. Obtained on July 9th, 2018, from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
3. Dalpiaz, D. (2018). *Applied Statistics with R*. Obtained on July 9th, 2018, from: https://daviddalpiaz.github.io/appliedstats/
4. De Cock, Dean. (2011). *Ames, Iowa house price dataset*. Obtained on July 9th, 2018, from: http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls
5. De Cock, Dean. (2011). *Ames, Iowa house price dataset description*. Obtained on July 9th, 2018, from: https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt