---
title: "Scratch pad for Colinearity Analysis"
author: "Javier Matias-Cabrera, javierm4"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4)

```

```{r message=FALSE, warning=FALSE}
#options(repos=c(CRAN="<something sensible near you>"))
#install.packages("jtools")
#install.packages("emmeans")
library(lmtest)
library(jtools)
library(emmeans)

source("load_dataset.r")
source("transformations.r")
source("plots.r")

```


## Interaction Analysis

Load the dataset and split it into training and testing data sets.
```{r message=FALSE, warning=FALSE}
housing_data = read_dataset("../data/AmesHousing.csv")

splits = splitDataset(housing_data)
training_data = splits$train
test_data = splits$test
```



I would like to know if the living area of a house is more valuable in some neighborhoods than others. We'll do an ANOVA for the interaction between these two predictors.
```{r}
interaction = aov(SalePrice ~  Neighborhood * Gr.Liv.Area, data=housing_data)
model = lm(SalePrice ~  Neighborhood * Gr.Liv.Area, data=housing_data)
(summary = summary(interaction)[[1]])
```

P-value of the interaction between Neighborhood and Living Area appears is very small (`r  summary[3,5]`), meaning that different Neighborhoods appear to place a higher value on living space.

We'll plot the slopes of the 6 neighborhoods with the largest amount of observations.
```{r}
ordered_neigh = order(-table(housing_data$Neighborhood))
interact_plot(model, pred=Gr.Liv.Area, modx=Neighborhood, color.class="Qual1",
              modxvals = names(table(housing_data$Neighborhood)[ordered_neigh][1:6]))

```

We notice that the slops are different depending on the neighborhood. We'll do slope analysis to see which of the Gr.Liv.Area slopes are significant.
```{r}
(slopes = sim_slopes(model, pred=Gr.Liv.Area, modx=Neighborhood, johnson_neyman = FALSE,
                     centered= "none", confint=T))
```


## Finding orthogonal predictors

We'll first fit a simple additive model on the training data.

```{r Stepwise-Search}
model_add = lm(SalePrice ~ . , data=training_data)
sigma(model_add) ## Residual standard error
summary(model_add)$adj.r.squared
```

We'll reduce the number of predictors by removing predictors that are colinear using stepwise search.

```{r}
n = nrow(housing_data)
model = step(model_add, direction = "both", trace = 0, k = log(n))
stepwise_search_formula = formula(model)
```

We'll train a model using the selected predictors on the training data only.
```{r warning=FALSE}
model_selected = lm(stepwise_search_formula, data=training_data)

model_selected$xlevels[["Kitchen.Qual"]] <- union(model_selected$xlevels[["Kitchen.Qual"]],
                                           levels(test_data$Kitchen.Qual))

### Residual Standard Error on training data
sigma(model_selected)

num_predictors = length(coef(model_selected))-1

### Residual Standard Error on test data
sqrt(sum((test_data$SalePrice - predict(model_selected, newdata=test_data))^2) / (nrow(test_data)-num_predictors) )

```
